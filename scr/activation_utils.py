# -*- coding: utf-8 -*-
"""activation_utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AG5x8u6U3UR6abLFR908sbgaQBVeyXhn
"""

"""
Утилиты для работы с активациями модели.
"""

import torch
import numpy as np
from typing import Dict, List, Tuple, DefaultDict
from collections import defaultdict
from tqdm import tqdm


def collect_all_activations_simultaneously(model, tokenizer, texts, layer_type='both'):
    """
    Собирает MLP и Attention активации одновременно.

    Args:
        model: Модель
        tokenizer: Токенизатор
        texts: Список текстов
        layer_type: 'mlp', 'attn' или 'both'

    Returns:
        Кортеж (mlp_activations, attention_activations)
    """
    mlp_activations = defaultdict(list) if layer_type in ['mlp', 'both'] else {}
    attention_activations = defaultdict(list) if layer_type in ['attn', 'both'] else {}

    hooks = []

    def make_mlp_hook(layer_num):
        def hook_fn(module, input, output):
            mlp_activations[layer_num].append(output[:, -1, :].detach().cpu())
        return hook_fn

    def make_attention_hook(layer_num):
        def hook_fn(module, input, output):
            act = output[0] if isinstance(output, tuple) else output
            attention_activations[layer_num].append(act[:, -1, :].detach().cpu())
        return hook_fn

    # Регистрация хуков
    for layer_num in range(len(model.model.layers)):
        layer = model.model.layers[layer_num]

        if layer_type in ['mlp', 'both']:
            mlp_hook = layer.mlp.register_forward_hook(make_mlp_hook(layer_num))
            hooks.append(mlp_hook)

        if layer_type in ['attn', 'both']:
            attn_hook = layer.self_attn.register_forward_hook(make_attention_hook(layer_num))
            hooks.append(attn_hook)

    # Сбор активаций
    for text in tqdm(texts, desc="Collecting activations"):
        inputs = tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(model.device)

        with torch.no_grad():
            _ = model(**inputs)

    # Удаление хуков
    for hook in hooks:
        hook.remove()

    # Конвертация в тензоры
    mlp_result = {}
    if mlp_activations:
        for layer in mlp_activations:
            if mlp_activations[layer]:
                mlp_result[layer] = torch.cat(mlp_activations[layer], dim=0)

    attn_result = {}
    if attention_activations:
        for layer in attention_activations:
            if attention_activations[layer]:
                attn_result[layer] = torch.cat(attention_activations[layer], dim=0)

    return mlp_result, attn_result


def compute_steering_vectors(pos_activations, neg_activations):
    """
    Вычисляет стиринговые векторы из активаций.

    Args:
        pos_activations: Активации для положительных примеров
        neg_activations: Активации для отрицательных примеров

    Returns:
        Словарь {layer: steering_vector}
    """
    steering_vectors = {}

    for layer in pos_activations:
        if layer in neg_activations:
            if pos_activations[layer] is not None and neg_activations[layer] is not None:
                pos_tensor = pos_activations[layer]
                neg_tensor = neg_activations[layer]
                pos_mean = pos_tensor.mean(dim=0)
                neg_mean = neg_tensor.mean(dim=0)
                steering_vector = pos_mean - neg_mean
                steering_vectors[layer] = steering_vector

    return steering_vectors


def calculate_norms_from_data(steering_vectors):
    """
    Вычисляет нормы стиринговых векторов.

    Args:
        steering_vectors: Словарь стиринговых векторов

    Returns:
        Словарь {layer: norm}
    """
    norms = {}

    for layer in sorted(steering_vectors.keys()):
        vector = steering_vectors[layer]
        vector_f32 = vector.float()
        norm = torch.norm(vector_f32).item()
        norms[layer] = norm

        print(f"Слой {layer:2d}: норма = {norm:.6f}")

    return norms


def clear_all_hooks(model):
    """Очищает все хуки в модели."""
    for name, module in model.named_modules():
        if hasattr(module, '_forward_hooks'):
            module._forward_hooks.clear()
        if hasattr(module, '_forward_pre_hooks'):
            module._forward_pre_hooks.clear()
        if hasattr(module, '_backward_hooks'):
            module._backward_hooks.clear()