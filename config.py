# -*- coding: utf-8 -*-
"""config.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AG5x8u6U3UR6abLFR908sbgaQBVeyXhn
"""

import torch

MODEL_CONFIG = {
    "model_name": "NousResearch/Llama-2-7b-chat-hf",
    "torch_dtype": torch.float16,
    "device_map": "auto",
    "low_cpu_mem_usage": True
}

STEERING_CONFIG = {
    "layers": list(range(31, 32)),
    "read_token_index": -1,
    "move_to_cpu": True,
    "show_progress": True
}

EDITOR_CONFIG = {
    "system_prompt": "You are a helpful assistant who answers questions directly.",
    "default_layers": [31, 32],
    "bos_token": "<s>",
    "eos_token": "</s>",
    "b_inst": "[INST]",
    "e_inst": "[/INST]",
    "b_sys": "<<SYS>>",
    "e_sys": "<</SYS>>"
}

GENERATION_CONFIG = {
    "default": {
        "max_new_tokens": 50,
        "do_sample": True,
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 50,
        "repetition_penalty": 1.1
    },
    "aggressive": {
        "trigger_threshold": 0.4,
        "steering_strength": 1.6,
        "temperature": 0.3,
        "top_p": 0.9,
        "repetition_penalty": 1.2
    }
}

PARAMS_GRID = [
    {'trigger_threshold': 0.6, 'steering_strength': 1.2, 'temperature': 0.5, 'top_p': 0.9},
    {'trigger_threshold': 0.5, 'steering_strength': 1.4, 'temperature': 0.3, 'top_p': 0.9},
    {'trigger_threshold': 0.7, 'steering_strength': 1.0, 'temperature': 0.3, 'top_p': 0.9},
    {'trigger_threshold': 0.4, 'steering_strength': 1.0, 'temperature': 0.7, 'top_p': 0.9},
]

PATH_CONFIG = {
    "data_dir": "data",
    "models_dir": "models",
    "results_dir": "results",
    "logs_dir": "results/logs",
    "eval_dir": "results/evaluations"
}