# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AG5x8u6U3UR6abLFR908sbgaQBVeyXhn
"""

"""
Основной скрипт для запуска факт-эдитинга.
"""

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import config
from src.data_utils import load_counterfact_data, prepare_counterfact_pairs
from src.fact_editor import FactEditorFromPairs
from src.activation_utils import clear_all_hooks


def load_model():
    """Загрузка модели и токенизатора."""
    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_CONFIG["model_name"])
    model = AutoModelForCausalLM.from_pretrained(
        config.MODEL_CONFIG["model_name"],
        torch_dtype=config.MODEL_CONFIG["torch_dtype"],
        device_map=config.MODEL_CONFIG["device_map"],
        low_cpu_mem_usage=config.MODEL_CONFIG["low_cpu_mem_usage"],
    )
    return tokenizer, model


def setup_editor_for_facts(editor, data_pairs, prompts_list, num_facts=10):
    """Настройка редактора для фактов."""

    for i in range(min(num_facts, len(data_pairs))):
        prompt_data = prompts_list[i]
        pair = data_pairs[i]
        fact_id = f"fact_{i}"

        editor.setup_from_pair(
            fact_id=fact_id,
            pos_prompt=pair[0],
            neg_prompt=pair[1],
            subject=prompt_data['subject'],
            true_value=prompt_data['positive'],
            new_value=prompt_data['negative'],
            layers=config.EDITOR_CONFIG["default_layers"]
        )

    return editor


def test_editor(editor, data_pairs, prompts_list, num_tests=5):
    """Тестирование редактора."""

    for i in range(min(num_tests, len(data_pairs))):

        prompt_data = prompts_list[i]
        pair = data_pairs[i]
        fact_id = f"fact_{i}"

        test_prompt = pair[0].replace("(A)", "")

        print(f"\nСубъект: {prompt_data['subject']}")
        print(f"Вопрос: {prompt_data['question']}")
        print(f"Ожидаемый ответ: {prompt_data['negative']}")

        print("\nБез стиринга:")
        try:
            baseline = editor.edit_generation(
                test_prompt, fact_id,
                trigger_threshold=1.0,
                steering_strength=0.0,
                max_new_tokens=50,
                temperature=0.7
            )
            print(f"Ответ: {baseline[:100]}")
        except Exception as e:
            print(f"Ошибка: {e}")

        print("\nСо стирингом:")
        try:
            edited = editor.edit_generation(
                test_prompt, fact_id,
                **config.GENERATION_CONFIG["aggressive"],
                max_new_tokens=50
            )
            print(f"Ответ: {edited[:200]}")

        except Exception as e:
            print(f"Ошибка: {e}")

def main():
    tokenizer, model = load_model()
    counterfact_data = load_counterfact_data("data/counterfact.json")
    data_pairs = prepare_counterfact_pairs(counterfact_data)

    prompts_list = []
    for example in counterfact_data:
        prompt_template = example['requested_rewrite']['prompt']
        subject = example['requested_rewrite']['subject']

        if '{}' in prompt_template:
            question = prompt_template.format(subject)
        else:
            question = prompt_template

        positive = example['requested_rewrite']['target_true']['str']
        negative = example['requested_rewrite']['target_new']['str']

        prompts_list.append({
            'question': question,
            'positive': positive,
            'negative': negative,
            'subject': subject
        })

    editor = FactEditorFromPairs(model, tokenizer)
    editor = setup_editor_for_facts(editor, data_pairs, prompts_list, num_facts=50)
    test_editor(editor, data_pairs, prompts_list, num_tests=3)
    clear_all_hooks(model)

if __name__ == "__main__":
    main()